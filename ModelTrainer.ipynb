{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for AnDi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime # for timing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity I will include all classes and functions used in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates and early stopping object\n",
    "#see: https://github.com/Bjarten/early-stopping-pytorch\n",
    "#used for training the model\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code modified from:\n",
    "#https://github.com/Bjarten/early-stopping-pytorch/blob/master/MNIST_Early_Stopping_example.ipynb\n",
    "#modified to add drop_last = True so that data size doesnt\n",
    "#have to exactly dividsible by the batch size\n",
    "\n",
    "def create_loaders(train_data, test_data, batch_size):\n",
    "\n",
    "    # percentage of training set to use as validation\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # load training data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=0,\n",
    "                                               drop_last=True)\n",
    "    \n",
    "    # load validation data in batches\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=valid_sampler,\n",
    "                                               num_workers=0,\n",
    "                                               drop_last=True)\n",
    "    \n",
    "    # load test data in batches\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=0,\n",
    "                                              drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConejeroConvNet(nn.Module):\n",
    "    def __init__(self, input_size, batch_size, hidden_size, device = 'cuda', output_size = 5):\n",
    "        super().__init__()#can call like this as of python3\n",
    "\n",
    "        #initialize some parameters\n",
    "        self.input_size = input_size #length of seq or trajectory. also known as feature\n",
    "        self.hidden_size = hidden_size #same as num neuron\n",
    "        self.batch_size = batch_size \n",
    "        self.device = device\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        #more parameters for tuning\n",
    "        self.conv_kernel_size = 3\n",
    "        self.drop_prob = .2\n",
    "        self.mp_kernel_size = 2\n",
    "        self.num_lstm_layers = 3\n",
    "        \n",
    "        #do the Layers----------------\n",
    "\n",
    "        #convolutional layers\n",
    "        #Dropout\n",
    "        #introduced between 2 fully connected layers to introduce non-linearity\n",
    "        #and reduce over fitting\n",
    "        #outputs are scaled by factor 1/1-p  \n",
    "        #MaxPooling\n",
    "        #reduces the speatial size of the convolved features\n",
    "        #helps to reduce over fitting \n",
    "        \n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 1, out_channels = 20, kernel_size = self.conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = 20 , out_channels = 64, kernel_size = self.conv_kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.drop_prob),\n",
    "            nn.MaxPool1d(kernel_size=self.mp_kernel_size)\n",
    "        )\n",
    "        conBlockOutDim = int(64/2)*int((self.input_size-2*self.conv_kernel_size+2))\n",
    "        \n",
    "        \n",
    "        #add lstm layer\n",
    "        self.bi_lstm = nn.LSTM(input_size=int((self.input_size-2*self.conv_kernel_size+2)/2), \n",
    "                               hidden_size= self.hidden_size, bidirectional = True, \n",
    "                               num_layers = self.num_lstm_layers,\n",
    "                               batch_first = True)\n",
    "        \n",
    "          #LinearLayers\n",
    "        self.linearOuts = nn.Sequential(\n",
    "            nn.Linear(2*self.hidden_size*2*self.batch_size, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, self.output_size)\n",
    "        )#two hidden get concatenated\n",
    "        \n",
    "    def init_hidden(self): #num batch is number of batches not batch size\n",
    "        # (num_layers, batch_size, n_neurons)\n",
    "        #using one instead of num_batch becuase I can only process \n",
    "        #onebatch at a time to do the fast weights\n",
    "        return (torch.zeros(2*self.num_lstm_layers, self.batch_size, self.hidden_size))\n",
    "               \n",
    "    def forward(self, X):\n",
    "        #initialize hidden and cellState\n",
    "        self.hidden = self.init_hidden().to(device)\n",
    "        self.cellState = self.init_hidden().to(device)\n",
    "        \n",
    "        \n",
    "        #convolutional\n",
    "        out = self.ConvBlock(X)\n",
    "        \n",
    "        #add lstm layer\n",
    "        #print('convblock out dim: ', out.size())\n",
    "        out, (self.hidden, self.cellState) = self.bi_lstm(out)\n",
    "        #print('out size: ', out.size())\n",
    "        #reshape so I can feed to linear layer\n",
    "        out = out.contiguous().view(out.size(0),-1)\n",
    "        #print('out size: ', out.size())\n",
    "        #linear Layer with RELU activation function\n",
    "        out = self.linearOuts(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### this code here is for importing data for training\n",
    "#csv file should be csv file from competition i.e. task2 dimension 1 \n",
    "#the csv should have the first column(dimension ) removed\n",
    "#the csv is already padded to max length of 1000\n",
    "#read in the data from the csv\n",
    "task = torch.from_numpy(np.genfromtxt('300KT2D1task2_2.csv', delimiter = ',')).unsqueeze(1)\n",
    "#task data will have form torch.Size([189810, 1, 1000]) where 189810 is number of trajectories\n",
    "#name is misleading because I ran out of RAM when writing 300k actually has only 189k trajectories\n",
    "\n",
    "#simply remove the first column(dimension)\n",
    "ref = torch.from_numpy(np.genfromtxt('300KT2D1ref2_2.csv', delimiter = ','))\n",
    "\n",
    "#too add more training data simply import more and run the loop to add it to the tuple\n",
    "#I found ~300k trajectories to be optimal after that model training did not improve\n",
    "\n",
    "\n",
    "#make tuple of the data\n",
    "#the tuple has the task and reference data\n",
    "#the list of tuple will be passed to a dataLoader for training\n",
    "data = []\n",
    "for i in range(len(ref)):\n",
    "    data.append((task[i], ref[i]))\n",
    "\n",
    "    \n",
    "    \n",
    "#split into train and test data \n",
    "BATCH_SIZE = 32\n",
    "train = data[0:150000] #used through 75% for train 25% test\n",
    "test = data[150000:] \n",
    "\n",
    "train_loader, test_loader, valid_loader  = create_loaders(train, test, BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "#will check for GPU if found device will be set to 'cuda' else it will be set to 'cpu'\n",
    "#check for gpu code from: https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConejeroConvNet(\n",
       "  (ConvBlock): Sequential(\n",
       "    (0): Conv1d(1, 20, kernel_size=(3,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(20, 64, kernel_size=(3,), stride=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bi_lstm): LSTM(498, 32, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (linearOuts): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1000, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=50, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try the module\n",
    "#try the module\n",
    "HIDDEN_SIZE = 32\n",
    "LEARN_RATE = .001\n",
    "NUM_EPOCH = 100\n",
    "BATCH_SIZE=BATCH_SIZE #defined above\n",
    "\n",
    "\n",
    "#start the training\n",
    "# Model instance\n",
    "#device = torch.device('cpu')\n",
    "model = ConejeroConvNet(input_size= 1000, batch_size=BATCH_SIZE,\n",
    "                        hidden_size=HIDDEN_SIZE,\n",
    "                        output_size=5, device = device)\n",
    "\n",
    "model.to(device) #sends the model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARN_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to train the model\n",
    "#function modified from: \n",
    "#see: https://github.com/Bjarten/early-stopping-pytorch\n",
    "def train_model(model, train_loader, test_loader, valid_loader, batch_size, patience, n_epochs, device):\n",
    "    print('DEVICE: ', device)\n",
    "    #modified from: https://github.com/Bjarten/early-stopping-pytorch/blob/master/MNIST_Early_Stopping_example.ipynb\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, dat in enumerate(train_loader, 1):\n",
    "            data = dat[0].to(device).float()#must be float\n",
    "            target = dat[1].to(device).long()#must cast to long\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            #print('made it past model in put')\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for dataV, targetV in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(dataV.to(device).float())\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, targetV.to(device).long())\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cuda\n",
      "[  1/200] train_loss: 1.14736 valid_loss: 0.92465\n",
      "Validation loss decreased (inf --> 0.924648).  Saving model ...\n",
      "[  2/200] train_loss: 0.78689 valid_loss: 0.68925\n",
      "Validation loss decreased (0.924648 --> 0.689245).  Saving model ...\n",
      "[  3/200] train_loss: 0.64999 valid_loss: 0.59310\n",
      "Validation loss decreased (0.689245 --> 0.593104).  Saving model ...\n",
      "[  4/200] train_loss: 0.58235 valid_loss: 0.55701\n",
      "Validation loss decreased (0.593104 --> 0.557013).  Saving model ...\n",
      "[  5/200] train_loss: 0.54885 valid_loss: 0.55926\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[  6/200] train_loss: 0.51817 valid_loss: 0.54657\n",
      "Validation loss decreased (0.557013 --> 0.546573).  Saving model ...\n",
      "[  7/200] train_loss: 0.49788 valid_loss: 0.52402\n",
      "Validation loss decreased (0.546573 --> 0.524023).  Saving model ...\n",
      "[  8/200] train_loss: 0.47800 valid_loss: 0.51224\n",
      "Validation loss decreased (0.524023 --> 0.512244).  Saving model ...\n",
      "[  9/200] train_loss: 0.46276 valid_loss: 0.54483\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 10/200] train_loss: 0.44834 valid_loss: 0.50550\n",
      "Validation loss decreased (0.512244 --> 0.505502).  Saving model ...\n",
      "[ 11/200] train_loss: 0.43511 valid_loss: 0.51621\n",
      "EarlyStopping counter: 1 out of 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-249a3743d864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#run model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m mod, trainLoss, validLoss = train_model(model = model,train_loader= train_loader, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                         \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                         patience = 20, n_epochs=200, device = device)\n",
      "\u001b[0;32m<ipython-input-14-f8016862539a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, valid_loader, batch_size, patience, n_epochs, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# record training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run model training\n",
    "mod, trainLoss, validLoss = train_model(model = model,train_loader= train_loader, \n",
    "                                        test_loader = test_loader, valid_loader = valid_loader,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        patience = 20, n_epochs=200, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "#see: https://github.com/Bjarten/early-stopping-pytorch\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "num_classes = 5\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "classes = list(i+1 for i in range(num_classes))\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data = data.to(device).float()\n",
    "    target = target.to(device).long()\n",
    "    \n",
    "    if len(target.data) != BATCH_SIZE:\n",
    "        break\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(BATCH_SIZE):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    print('classTotal: ', class_total[i])\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of Class %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i+1), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))#not sure what this does\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beeps when the training process is done\n",
    "duration = 2   # seconds\n",
    "freq = 440  # Hz\n",
    "\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
